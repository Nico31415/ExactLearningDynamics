{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEA: DO EVERYTHING FOR BATCH SIZE = 1 FIRST, THEN EXTEND TO BIGGER BATCH SIZE. THIS IS BECAUSE IT IS EASIER TO DEAL WITH VECTORS RATHER THAN MATRICES\n",
    "\n",
    "https://stanford.edu/~rezab/dao/notes/L08/cme323_lec8.pdf\n",
    "\n",
    "### Definition: Convexity\n",
    "\n",
    "A function $f: \\R^n \\rightarrow \\R$ is convex if for all $x, y \\in \\R^n$, for all $\\lambda \\in [0, 1]$:\n",
    "\n",
    "$f(x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda) f(y)$\n",
    "\n",
    "This is equivalent to:\n",
    "\n",
    "$f(x) + \\nabla f(x)^T (x-y) \\leq f(y)$\n",
    "\n",
    "And \n",
    "\n",
    "$\\nabla^2 f(x) \\geq 0$ (positive semidefinite)\n",
    "\n",
    "\n",
    "\n",
    "### Definition Strong Convexity:\n",
    "\n",
    "A function $f: \\R^n \\rightarrow \\R$ is $\\mu$ strongly-convex if the function:\n",
    "\n",
    "$f(x) - \\frac{\\mu}{2} ||x||_2^2$\n",
    "\n",
    "is convex.\n",
    "\n",
    "If $f$ is twice differentiable, $f$ is $\\mu$-strongly convex if and only if for all $z, x \\in \\R^n$\n",
    "\n",
    "$z^T \\nabla^2 f(x) z \\geq \\mu ||z||_2^2$\n",
    "\n",
    "### L-smooth function\n",
    "A differentiable function $f: \\R^n \\rightarrow \\R$ is said to be L-smooth if for all $x, y \\in \\R^n$ we have that:\n",
    "\n",
    "$||\\nabla f(x) - \\nabla f(y) ||_2 \\leq L ||x - y||_2$ \n",
    "\n",
    "(taken from: https://www.stat.cmu.edu/~ryantibs/convexopt-F13/scribes/lec6.pdf)\n",
    "### Theorem: Convex Gradient Descent:\n",
    "\n",
    "Suppose the function $f: \\R^n -> \\R$ is convex and differentiable, and that its gradient is Lipschitz continuous with constant L > 0. Then if we run gradient descent for k iterations with a fixed stepsize $t \\leq L$ it will yield a solution $f^{(k)}$ that satisfies:\n",
    "\n",
    "$f(x_k) - f_* \\leq \\frac{||x_0 - x_*||_2^2}{2 t K} \\leq L \\frac{||x_0 - x_*||_2^2}{2 K}$\n",
    "\n",
    "Where $f_*$ is the optimal value. Intuitively this means that gradient descent is guaranteed to converge and that it converges with rate $O(\\frac{1}{k})$\n",
    "\n",
    "Consequently, we require $K \\geq L \\frac{||x_0 - x_*||_2^2}{2 \\epsilon}$ to find an $\\epsilon$ optimal point.\n",
    "\n",
    "\n",
    "### Theorem: Strongly Convex Gradient Descent:\n",
    "\n",
    "Let $f: \\R^n \\rightarrow \\R$ be a L-smooth, $\\mu$-strongly convex function for $\\mu > 0$. Then for $x_0 \\in \\R^n$ Let $x_{k+1} = x_k - \\frac{1}{L} \\nabla f(x_k)$ for all $k \\geq 0$. Then,\n",
    "\n",
    "$ f(x_k) - f_* \\leq (1 - \\frac{\\mu}{L})^k (f(x_0) - f_*)$\n",
    "\n",
    "Consequently, we require $K \\geq \\frac{L}{\\mu} log(\\frac{f(x_0) - f_*}{\\epsilon})$ to find an $\\epsilon$ optimal point. (SEE LINKED PAPER FOR PROOF DETAILS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "in scalar setting depending on how close the numbers are if it is a scalar, not much you can do or reuse\n",
    "\n",
    "if the weights are aligned you can save some learning steps. (W1 @ W2.T)\n",
    "\n",
    "loss of task 1\n",
    "\n",
    "curriculum: you learn something in order to learn something else better on. \n",
    "\n",
    "how do i need to learn the first task to be able to learn quickly the second task\n",
    "\n",
    "\n",
    "for continual learning i want to minimise the sum of the losses on each task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider the cost function in our problem:\n",
    "\n",
    "We have $X \\in \\R^{(in \\times batch)}$, $Y \\in \\R^{(out \\times batch)}$, $W \\in \\R^{(out \\times batch)}$. (We are ignoring hidden_dim as for now we only care about the whole W)\n",
    "\n",
    "The cost function is defined as:\n",
    "\n",
    "$f(W) = \\frac{1}{2 P} ||W X - Y||_F^2$\n",
    "\n",
    "With $P$ = batch_size.\n",
    "\n",
    "This function takes in a matrix, which makes the problem more complicated. For now, let's consider the case when out_dim = 1. Let $w \\in R^{in} := W^T$. Also for even more simplicity suppose batch_size = 1. Then we can define the cost function:\n",
    "\n",
    "$f(w) = \\frac{1}{2 P} ||w^T X - Y||_F ^2$\n",
    "$f(w) = \\frac{1}{2} ||w^T X - Y||_2^2$\n",
    "\n",
    "Since the expression inside the F norm is a vector not a matrix so these two norms are equivalent. \n",
    "\n",
    "Now, consider:\n",
    "\n",
    "$\\nabla f(w) = (w^T X - Y) X^T$\n",
    "\n",
    "$\\nabla f(w) = w^T X X^T - Y X^T$\n",
    "\n",
    "$\\nabla f(w) = w^T - Y X^T$ (WHITENED INPUTS ASSUMPTION, BATCH_SIZE = 1)\n",
    "\n",
    "$\\nabla^2 f(w) = I$, which is positive definite, so $f(w)$ is convex. \n",
    "\n",
    "In addition, we can see that for any $z \\in R^{in}$, \n",
    "\n",
    "$ z^T \\nabla^2 f(w) z = z^T z = ||z||_2^2 \\geq \\mu ||z||_2^2$ for $\\mu \\leq 1$\n",
    "\n",
    "Hence, f is also $\\mu$ strongly convex for $\\mu = 1$\n",
    "\n",
    "We can also see that:\n",
    "\n",
    "$|| \\nabla f(w_1) - \\nabla f(w_2) || = || w_1^T - Y X^T - w_2^T + Y X^T|| = ||w_1^T - w_2^T|| \\leq ||w_1^T - w_2^T||$ so L = 1. \n",
    "\n",
    "Hence we can use the theorem for strongly convex functions:\n",
    "\n",
    "We require $K \\geq log(\\frac{f(x_0) - f_*}{\\epsilon})$ to find an $\\epsilon$ optimal point.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prove a similar result for batch_size > 1:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prove a similar result for out_dim > 1:\n",
    "\n",
    "Fobrenius norm is just a sum of L2 norms (CHECK) so should be quite straightforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHOW THAT THIS ACTUALLY HOLDS IN SIMULATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT DOES THIS TELL US ABOUT THE CONTINUAL LEARNING SETTING?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove MSE is convex and strongly convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence properties for Gradient Descent on convex and strongly convex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalise this to batch size > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us about task similarity in the continual learning setting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
